import os
import sys

cwd = os.getcwd()
sys.path.append( cwd )
new_path = [ os.path.join( cwd, "scripts" ),
             os.path.join( cwd, "lib" ),
             os.path.join( cwd, 'test' ),
             os.path.join( cwd, 'scripts', 'api' ) ]
new_path.extend( sys.path )
sys.path = new_path

from galaxy import eggs
eggs.require( 'mercurial' )
eggs.require( "nose" )

import logging
import install_and_test_tool_shed_repositories.base.test_db_util as test_db_util
import install_and_test_tool_shed_repositories.functional.test_install_repositories as test_install_repositories
import nose
import platform
import time
import tool_shed.repository_types.util as rt_util
import tool_shed.util.shed_util_common as suc
import urllib

from base.tool_shed_util import log_reason_repository_cannot_be_uninstalled

from common import update

from galaxy.util import asbool
from galaxy.util import listify
from galaxy.util import unicodify
from galaxy.util.json import from_json_string
from nose.plugins import Plugin
from tool_shed.util import tool_dependency_util

from tool_shed.util.xml_util import parse_xml

from mercurial import hg
from mercurial import ui

log = logging.getLogger(__name__)

# Set up a job_conf.xml that explicitly limits jobs to 10 minutes.
job_conf_xml = '''<?xml version="1.0"?>
<!-- A test job config that explicitly configures job running the way it is configured by default (if there is no explicit config). -->
<job_conf>
    <plugins>
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner" workers="4"/>
    </plugins>
    <handlers>
        <handler id="main"/>
    </handlers>
    <destinations>
        <destination id="local" runner="local"/>
    </destinations>
    <limits>
        <limit type="walltime">00:10:00</limit>
    </limits>
</job_conf>
'''

# Create a blank shed_tool_conf.xml to define the installed repositories.
shed_tool_conf_xml_template = '''<?xml version="1.0"?>
<toolbox tool_path="${shed_tool_path}">
</toolbox>
'''

# Since we will be running functional tests we'll need the upload tool, but the rest can be omitted.
tool_conf_xml = '''<?xml version="1.0"?>
<toolbox>
    <section name="Get Data" id="getext">
        <tool file="data_source/upload.xml"/>
    </section>
</toolbox>
'''

# Set up an empty shed_tool_data_table_conf.xml.
tool_data_table_conf_xml_template = '''<?xml version="1.0"?>
<tables>
</tables>
'''

# Optionally set the environment variable GALAXY_INSTALL_TEST_TOOL_SHEDS_CONF to the location of a
# tool shed's configuration file that includes the tool shed from which repositories will be installed.
tool_sheds_conf_xml = '''<?xml version="1.0"?>
<tool_sheds>
    <tool_shed name="Galaxy main tool shed" url="http://toolshed.g2.bx.psu.edu/"/>
    <tool_shed name="Galaxy test tool shed" url="http://testtoolshed.g2.bx.psu.edu/"/>
</tool_sheds>
'''

# Should this serve static resources (scripts, images, styles, etc.)?
STATIC_ENABLED = True

# If we have a tool_data_table_conf.test.xml, set it up to be loaded when the UniverseApplication is started.
# This allows one to specify a set of tool data that is used exclusively for testing, and not loaded into any
# Galaxy instance. By default, this will be in the test-data-repo/location directory generated by buildbot_setup.sh.
if os.path.exists( 'tool_data_table_conf.test.xml' ):
    additional_tool_data_tables = 'tool_data_table_conf.test.xml'
    additional_tool_data_path = os.environ.get( 'GALAXY_INSTALL_TEST_EXTRA_TOOL_DATA_PATH',
                                                os.path.join( 'test-data-repo', 'location' ) )
else:
    additional_tool_data_tables = None
    additional_tool_data_path = None

# Set up default tool data tables.
if os.path.exists( 'tool_data_table_conf.xml' ):
    tool_data_table_conf = 'tool_data_table_conf.xml'
elif os.path.exists( 'tool_data_table_conf.xml.sample' ):
    tool_data_table_conf = 'tool_data_table_conf.xml.sample'
else:
    tool_data_table_conf = None

# The GALAXY_INSTALL_TEST_TOOL_SHED_URL and GALAXY_INSTALL_TEST_TOOL_SHED_API_KEY environment variables must be
# set for this script to work correctly.  If the value of GALAXY_INSTALL_TEST_TOOL_SHED_URL does not refer to one
# of the defaults, the GALAXY_INSTALL_TEST_TOOL_SHEDS_CONF must refer to a tool shed configuration file that contains
# a definition for that tool shed.
galaxy_tool_shed_url = os.environ.get( 'GALAXY_INSTALL_TEST_TOOL_SHED_URL', None )
tool_shed_api_key = os.environ.get( 'GALAXY_INSTALL_TEST_TOOL_SHED_API_KEY', None )
exclude_list_file = os.environ.get( 'GALAXY_INSTALL_TEST_EXCLUDE_REPOSITORIES', 'install_test_exclude.xml' )

if 'GALAXY_INSTALL_TEST_SECRET' not in os.environ:
    galaxy_encode_secret = 'changethisinproductiontoo'
    os.environ[ 'GALAXY_INSTALL_TEST_SECRET' ] = galaxy_encode_secret
else:
    galaxy_encode_secret = os.environ[ 'GALAXY_INSTALL_TEST_SECRET' ]

default_galaxy_test_port_min = 10000
default_galaxy_test_port_max = 10999
default_galaxy_test_host = '127.0.0.1'
# The following should be an actual value (not None).  If developers manually specify their
# tests to use the API it will not work unless a master API key is specified.
default_galaxy_master_api_key = 123456

testing_single_repository_dict = {}
if 'repository_name' in os.environ and 'repository_owner' in os.environ:
    testing_single_repository_dict[ 'name' ] = str( os.environ[ 'repository_name' ] )
    testing_single_repository_dict[ 'owner' ] = str( os.environ[ 'repository_owner' ] )
    if 'repository_revision' in os.environ:
        testing_single_repository_dict[ 'changeset_revision' ] = str( os.environ[ 'repository_revision' ] )
    else:
        testing_single_repository_dict[ 'changeset_revision' ] = None

# Test frameworks that use this utility module.
REPOSITORIES_WITH_TOOLS = 'repositories_with_tools'
TOOL_DEPENDENCY_DEFINITIONS = 'tool_dependency_definitions'

class ReportResults( Plugin ):
    '''Simple Nose plugin to record the IDs of all tests run, regardless of success.'''
    name = "reportresults"
    passed = {}

    def options( self, parser, env=os.environ ):
        super( ReportResults, self ).options( parser, env=env )

    def configure(self, options, conf):
        super( ReportResults, self ).configure( options, conf )
        if not self.enabled:
            return

    def addSuccess( self, test ):
        '''Only record test IDs that correspond to tool functional tests.'''
        if 'TestForTool' in test.id():
            test_id = test.id()
            # Rearrange the test ID to match the format that is produced in test_results.failures
            test_id_parts = test_id.split( '.' )
            fixed_test_id = '%s (%s)' % ( test_id_parts[ -1 ], '.'.join( test_id_parts[ :-1 ] ) )
            test_parts = fixed_test_id.split( '/' )
            owner = test_parts[ -4 ]
            name = test_parts[ -3 ]
            test_identifier = '%s/%s' % ( owner, name )
            if test_identifier not in self.passed:
                self.passed[ test_identifier ] = []
            self.passed[ test_identifier ].append( fixed_test_id )

    def getTestStatus( self, test_identifier ):
        if test_identifier in self.passed:
            passed_tests = self.passed[ test_identifier ]
            del self.passed[ test_identifier ]
            return passed_tests
        return []

def get_api_url( base, parts=[], params=None ):
    if 'api' in parts and parts.index( 'api' ) != 0:
        parts.pop( parts.index( 'api' ) )
        parts.insert( 0, 'api' )
    elif 'api' not in parts:
        parts.insert( 0, 'api' )
    url = suc.url_join( base, *parts )
    if params is not None:
        query_string = urllib.urlencode( params )
        url += '?%s' % query_string
    return url

def get_database_version( app ):
    '''
    This method returns the value of the version column from the migrate_version table, using the provided app's SQLAlchemy session to determine
    which table to get that from. This way, it's provided with an instance of a Galaxy UniverseApplication, it will return the Galaxy instance's
    database migration version. If a tool shed UniverseApplication is provided, it returns the tool shed's database migration version.
    '''
    sa_session = app.model.context.current
    result = sa_session.execute( 'SELECT version FROM migrate_version LIMIT 1' )
    # This query will return the following structure:
    # row = [ column 0, column 1, ..., column n ]
    # rows = [ row 0, row 1, ..., row n ]
    # The first column in the first row is the version number we want.
    for row in result:
        version = row[ 0 ]
        break
    return version

def get_latest_downloadable_changeset_revision( url, name, owner ):
    error_message = ''
    parts = [ 'api', 'repositories', 'get_ordered_installable_revisions' ]
    params = dict( name=name, owner=owner )
    api_url = get_api_url( base=url, parts=parts, params=params )
    changeset_revisions, error_message = json_from_url( api_url )
    if error_message:
        return None, error_message
    if changeset_revisions:
        return changeset_revisions[ -1 ], error_message
    else:
        return suc.INITIAL_CHANGELOG_HASH, error_message

def get_missing_tool_dependencies( repository ):
    log.debug( 'Checking %s repository %s for missing tool dependencies.' % ( repository.status, repository.name ) )
    missing_tool_dependencies = repository.missing_tool_dependencies
    for tool_dependency in repository.tool_dependencies:
        log.debug( 'Tool dependency %s version %s has status %s.' % ( tool_dependency.name, tool_dependency.version, tool_dependency.status ) )
    for repository_dependency in repository.repository_dependencies:
        if repository_dependency.includes_tool_dependencies:
            missing_tool_dependencies.extend( get_missing_tool_dependencies( repository_dependency ) )
    return missing_tool_dependencies

def get_repositories_to_install( tool_shed_url, test_framework ):
    """
    Get a list of repository info dicts to install. This method expects a json list of dicts with the following structure:
    [{ "changeset_revision": <revision>,
       "encoded_repository_id": <encoded repository id from the tool shed>,
       "name": <name>,
       "owner": <owner>,
       "tool_shed_url": <url> }]
    """
    error_message = ''
    latest_revision_only = '-check_all_revisions' not in sys.argv
    if latest_revision_only:
        log.debug( 'Testing is restricted to the latest downloadable revision in this test run.' )
    repository_dicts = []
    parts = [ 'repository_revisions' ]
    # We'll filter out deprecated repositories from testing since testing them is necessary only if reproducibility
    # is guaranteed and we currently do not guarantee reproducibility.
    if test_framework == REPOSITORIES_WITH_TOOLS:
        params = dict( do_not_test='false',
                       downloadable='true',
                       includes_tools='true',
                       malicious='false',
                       missing_test_components='false',
                       skip_tool_test='false' )
    elif test_framework == TOOL_DEPENDENCY_DEFINITIONS:
        params = dict( do_not_test='false',
                       downloadable='true',
                       malicious='false',
                       skip_tool_test='false' )
    api_url = get_api_url( base=tool_shed_url, parts=parts, params=params )
    baseline_repository_dicts, error_message = json_from_url( api_url )
    if error_message:
        return None, error_message
    for baseline_repository_dict in baseline_repository_dicts:
        # We need to get some details from the tool shed API, such as repository name and owner, to pass on to the
        # module that will generate the install methods.
        repository_dict, error_message = \
            get_repository_dict( galaxy_tool_shed_url, baseline_repository_dict )
        if error_message:
            log.debug( 'Error getting additional details from the API: %s' % str(  error_message ) )
        else:
            # Don't test deprecated repositories since testing them is necessary only if reproducibility is guaranteed
            # and we are not currently guaranteeing reproducibility.
            deprecated = asbool( repository_dict.get( 'deprecated', False ) )
            if not deprecated:
                # Don't test empty repositories.
                changeset_revision = baseline_repository_dict[ 'changeset_revision' ]
                if changeset_revision != suc.INITIAL_CHANGELOG_HASH:
                    # If testing repositories of type tool_dependency_definition, filter accordingly.
                    if test_framework == TOOL_DEPENDENCY_DEFINITIONS and repository_dict[ 'type' ] != rt_util.TOOL_DEPENDENCY_DEFINITION:
                        continue
                    # Merge the dictionary returned from /api/repository_revisions with the detailed repository_dict and
                    # append it to the list of repository_dicts to install and test.
                    if latest_revision_only:
                        latest_revision = repository_dict[ 'latest_revision' ]
                        if changeset_revision == latest_revision:
                            repository_dicts.append( dict( repository_dict.items() + baseline_repository_dict.items() ) )
                    else:
                        repository_dicts.append( dict( repository_dict.items() + baseline_repository_dict.items() ) )
    if testing_single_repository_dict:
        tsr_name = testing_single_repository_dict[ 'name' ]
        tsr_owner = testing_single_repository_dict[ 'owner' ]
        tsr_changeset_revision = testing_single_repository_dict[ 'changeset_revision' ]
        log.debug( 'Testing single repository with name %s and owner %s.' % ( str( tsr_name ), str( tsr_owner ) ) )
        for repository_to_install in repository_dicts:
            rti_name = repository_to_install[ 'name' ]
            rti_owner = repository_to_install[ 'owner' ]
            rti_changeset_revision = repository_to_install[ 'changeset_revision' ]
            if rti_name == tsr_name and rti_owner == tsr_owner:
                if tsr_changeset_revision is None:
                    return [ repository_to_install ], error_message
                else:
                    if tsr_changeset_revision == rti_changeset_revision:
                        return repository_dicts, error_message
        return repository_dicts, error_message
    # Get a list of repositories to test from the tool shed specified in the GALAXY_INSTALL_TEST_TOOL_SHED_URL
    # environment variable.
    log.debug( "The Tool Shed's API url...\n%s" % str( api_url ) )
    log.debug( "...retrieved %d repository revisions for installation and possible testing." % len( repository_dicts ) )
    log.debug( "Repository revisions for testing:" )
    for repository_dict in repository_dicts:
        name = str( repository_dict.get( 'name', None ) )
        owner = str( repository_dict.get( 'owner', None ) )
        changeset_revision = str( repository_dict.get( 'changeset_revision', None ) )
        log.debug( "Revision %s of repository %s owned by %s" % ( changeset_revision, name, owner ) )
    return repository_dicts, error_message

def get_repository_current_revision( repo_path ):
    """This method uses the python mercurial API to get the current working directory's mercurial changeset hash."""
    # Initialize a mercurial repo object from the provided path.
    repo = hg.repository( ui.ui(), repo_path )
    # Get the working directory's change context.
    ctx = repo[ None ]
    # Extract the changeset hash of the first parent of that change context (the most recent changeset to which the
    # working directory was updated).
    changectx = ctx.parents()[ 0 ]
    # Also get the numeric revision, so we can return the customary id:hash changeset identifiers.
    ctx_rev = changectx.rev()
    hg_id = '%d:%s' % ( ctx_rev, str( changectx ) )
    return hg_id

def get_repository_dict( url, repository_dict ):
    error_message = ''
    parts = [ 'api', 'repositories', repository_dict[ 'repository_id' ] ]
    api_url = get_api_url( base=url, parts=parts )
    extended_dict, error_message = json_from_url( api_url )
    if error_message:
        return None, error_message
    name = str( extended_dict[ 'name' ] )
    owner = str( extended_dict[ 'owner' ] )
    latest_changeset_revision, error_message = get_latest_downloadable_changeset_revision( url, name, owner )
    if error_message:
        return None, error_message
    extended_dict[ 'latest_revision' ] = str( latest_changeset_revision )
    return extended_dict, error_message

def get_repository_tuple_from_elem( elem ):
    attributes = elem.attrib
    name = attributes.get( 'name', None )
    owner = attributes.get( 'owner', None )
    changeset_revision = attributes.get( 'changeset_revision', None )
    return ( name, owner, changeset_revision )

def get_static_settings():
    """
    Return a dictionary of the settings necessary for a Galaxy application to be wrapped in the static
    middleware.  This mainly consists of the file system locations of url-mapped static resources.
    """
    cwd = os.getcwd()
    static_dir = os.path.join( cwd, 'static' )
    #TODO: these should be copied from universe_wsgi.ini
    #TODO: static_enabled needed here?
    return dict( static_enabled = True,
                 static_cache_time = 360,
                 static_dir = static_dir,
                 static_images_dir = os.path.join( static_dir, 'images', '' ),
                 static_favicon_dir = os.path.join( static_dir, 'favicon.ico' ),
                 static_scripts_dir = os.path.join( static_dir, 'scripts', '' ),
                 static_style_dir = os.path.join( static_dir, 'june_2007_style', 'blue' ),
                 static_robots_txt = os.path.join( static_dir, 'robots.txt' ) )

def get_tool_test_results_dict( tool_test_results_dicts ):
    if tool_test_results_dicts:
        # Inspect the tool_test_results_dict for the last test run to make sure it contains only a test_environment
        # entry.  If it contains more entries, then the script ~/tool_shed/api/check_repositories_for_functional_tests.py
        # was not executed in preparation for this script's execution, so we'll just create an empty dictionary.
        tool_test_results_dict = tool_test_results_dicts[ 0 ]
        if len( tool_test_results_dict ) <= 1:
            # We can re-use the mostly empty tool_test_results_dict for this run because it is either empty or it contains only
            # a test_environment entry.  If we use it we need to temporarily eliminate it from the list of tool_test_results_dicts
            # since it will be re-inserted later.
            tool_test_results_dict = tool_test_results_dicts.pop( 0 )
        elif len( tool_test_results_dict ) == 2 and \
            'test_environment' in tool_test_results_dict and 'missing_test_components' in tool_test_results_dict:
            # We can re-use tool_test_results_dict if its only entries are "test_environment" and "missing_test_components".
            # In this case, some tools are missing tests components while others are not.
            tool_test_results_dict = tool_test_results_dicts.pop( 0 )
        else:
            # The latest tool_test_results_dict has been populated with the results of a test run, so it cannot be used.
            tool_test_results_dict = {}
    else:
        # Create a new dictionary for this test test run, 
        tool_test_results_dict = {}
    return tool_test_results_dict

def get_tool_test_results_dicts( tool_shed_url, encoded_repository_metadata_id ):
    """
    Return the list of dictionaries contained in the Tool Shed's repository_metadata.tool_test_results
    column via the Tool Shed API.
    """
    error_message = ''
    parts = [ 'api', 'repository_revisions', encoded_repository_metadata_id ]
    api_url = get_api_url( base=tool_shed_url, parts=parts )
    repository_metadata, error_message = json_from_url( api_url )
    if error_message:
        return None, error_message
    # The tool_test_results used to be stored as a single dictionary rather than a list, but we currently
    # return a list.
    tool_test_results = listify( repository_metadata.get( 'tool_test_results', [] ) )
    return tool_test_results, error_message

def get_webapp_global_conf():
    """Return the global_conf dictionary sent as the first argument to app_factory."""
    global_conf = {}
    if STATIC_ENABLED:
        global_conf.update( get_static_settings() )
    return global_conf

def handle_missing_dependencies( app, repository, missing_tool_dependencies, repository_dict, tool_test_results_dicts,
                                 tool_test_results_dict, results_dict, can_update_tool_shed ):
    """Handle missing repository or tool dependencies for an installed repository."""
    # If a tool dependency fails to install correctly, this should be considered an installation error,
    # and functional tests should be skipped, since the tool dependency needs to be correctly installed
    # for the test to be considered reliable.
    log.debug( 'The following dependencies of this repository are missing, so skipping functional tests.' )
    # In keeping with the standard display layout, add the error message to the dict for each tool individually.
    for dependency in repository.missing_tool_dependencies:
        name = str( dependency.name )
        type = str( dependency.type )
        version = str( dependency.version )
        error_message = unicodify( dependency.error_message )
        log.debug( 'Missing tool dependency %s of type %s version %s: %s' % ( name, type, version, error_message ) )
        test_result = dict( type=dependency.type,
                            name=dependency.name,
                            version=dependency.version,
                            error_message=dependency.error_message )
        tool_test_results_dict[ 'installation_errors' ][ 'tool_dependencies' ].append( test_result )
    for dependency in repository.missing_repository_dependencies:
        name = str( dependency.name )
        owner = str( dependency.owner )
        changeset_revision = str( dependency.changeset_revision )
        error_message = unicodify( dependency.error_message )
        log.debug( 'Missing repository dependency %s changeset revision %s owned by %s: %s' % \
            ( name, changeset_revision, owner, error_message ) )
        test_result = dict( tool_shed=dependency.tool_shed,
                            name=dependency.name,
                            owner=dependency.owner,
                            changeset_revision=dependency.changeset_revision,
                            error_message=dependency.error_message )
        tool_test_results_dict[ 'installation_errors' ][ 'repository_dependencies' ].append( test_result )
    # Record the status of this repository in the tool shed.
    params = dict( tools_functionally_correct=False,
                   do_not_test=False,
                   test_install_error=True )
    # TODO: do something useful with response_dict
    response_dict = register_test_result( galaxy_tool_shed_url,
                                          tool_test_results_dicts,
                                          tool_test_results_dict,
                                          repository_dict,
                                          params,
                                          can_update_tool_shed )
    # Since this repository is missing components we do not want to test it, uninstall it.
    uninstall_repository_and_repository_dependencies( app, repository_dict )
    results_dict[ 'repositories_failed_install' ].append( dict( name=str( repository.name ),
                                                                owner=str( repository.owner ),
                                                                changeset_revision=str( repository.changeset_revision ) ) )
    return results_dict

def initialize_results_dict( test_framework ):
    # Initialize a dictionary for the summary that will be printed to stdout.
    results_dict = {}
    if test_framework == REPOSITORIES_WITH_TOOLS:
        results_dict[ 'total_repositories_tested' ] = 0
        results_dict[ 'all_tests_passed' ] = []
        results_dict[ 'at_least_one_test_failed' ] = []
        results_dict[ 'repositories_failed_install' ] = []
    elif test_framework == TOOL_DEPENDENCY_DEFINITIONS:
        results_dict[ 'total_repositories_installed' ] = 0
        results_dict[ 'repositories_with_installation_error' ] = 0
        results_dict[ 'tool_dependencies_with_installation_error' ] = 0
    return results_dict

def initialize_tool_tests_results_dict( app, tool_test_results_dict ):
    test_environment_dict = tool_test_results_dict.get( 'test_environment', {} )
    if len( test_environment_dict ) == 0:
        # Set information about the tool shed to nothing since we cannot currently determine it from here.
        # We could eventually add an API method...
        test_environment_dict = dict( tool_shed_database_version='',
                                      tool_shed_mercurial_version='',
                                      tool_shed_revision='' )
    # Add the current time as the approximate time that this test run occurs.  A similar value will also be
    # set to the repository_metadata.time_last_tested column, but we also store it here because the Tool Shed
    # may be configured to store multiple test run results, so each must be associated with a time stamp.
    now = time.strftime( "%Y-%m-%d %H:%M:%S" )
    # Add information about the current platform.
    test_environment_dict[ 'time_tested' ] = now
    test_environment_dict[ 'python_version' ] = platform.python_version()
    test_environment_dict[ 'architecture' ] = platform.machine()
    operating_system, hostname, operating_system_version, uname, arch, processor = platform.uname()
    test_environment_dict[ 'system' ] = '%s %s' % ( operating_system, operating_system_version )
    # Add information about the current Galaxy environment.
    test_environment_dict[ 'galaxy_database_version' ] = get_database_version( app )
    test_environment_dict[ 'galaxy_revision' ] = get_repository_current_revision( os.getcwd() )
    # Initialize and populate the tool_test_results_dict.
    tool_test_results_dict[ 'test_environment' ] = test_environment_dict
    tool_test_results_dict[ 'passed_tests' ] = []
    tool_test_results_dict[ 'failed_tests' ] = []
    tool_test_results_dict[ 'installation_errors' ] = dict( current_repository=[],
                                                            repository_dependencies=[],
                                                            tool_dependencies=[] )
    return tool_test_results_dict

def install_repository( app, repository_dict ):
    """Install a repository defined by the received repository_dict from the tool shed into Galaxy."""
    name = str( repository_dict[ 'name' ] )
    owner = str( repository_dict[ 'owner' ] )
    changeset_revision = str( repository_dict[ 'changeset_revision' ] )
    error_message = ''
    repository = None
    log.debug( "Installing revision %s of repository %s owned by %s." % ( changeset_revision, name, owner ) )
    # Use the repository information dictionary to generate an install method that will install the repository into the
    # embedded Galaxy application, with tool dependencies and repository dependencies, if any.
    test_install_repositories.generate_install_method( repository_dict )
    # Configure nose to run the install method as a test.
    test_config = nose.config.Config( env=os.environ, plugins=nose.plugins.manager.DefaultPluginManager() )
    test_config.configure( sys.argv )
    # Run the configured install method as a test. This method uses the embedded Galaxy application's web interface to
    # install the specified repository with tool and repository dependencies also selected for installation.
    result, _ = run_tests( test_config )
    try:
        repository = test_db_util.get_installed_repository_by_name_owner_changeset_revision( name, owner, changeset_revision )
    except Exception, e:
        error_message = 'Error getting revision %s of repository %s owned by %s: %s' % ( changeset_revision, name, owner, str( e ) )
        log.exception( error_message )
    return repository, error_message

def is_latest_downloadable_revision( url, repository_dict ):
    name = str( repository_dict[ 'name' ] )
    owner = str( repository_dict[ 'owner' ] )
    changeset_revision = str( repository_dict[ 'changeset_revision' ] )
    latest_revision = get_latest_downloadable_changeset_revision( url, name=name, owner=owner )
    return changeset_revision == str( latest_revision )

def json_from_url( url ):
    error_message = ''
    url_handle = urllib.urlopen( url )
    url_contents = url_handle.read()
    try:
        parsed_json = from_json_string( url_contents )
    except Exception, e:
        error_message = str( url_contents )
        log.exception( 'Error parsing JSON data in json_from_url(): %s.' % str( e ) )
        return None, error_message
    return parsed_json, error_message

def parse_exclude_list( xml_filename ):
    """Return a list of repositories to exclude from testing."""
    # This method should return a list with the following structure:
    # [{ 'reason': The default reason or the reason specified in this section,
    #    'repositories': [( name, owner, changeset revision if changeset revision else None ),
    #                     ( name, owner, changeset revision if changeset revision else None )]}]
    exclude_list = []
    exclude_verbose = []
    xml_tree, error_message = parse_xml( xml_filename )
    if error_message:
        log.debug( 'The xml document %s defining the exclude list is invalid, so no repositories will be excluded from testing: %s' % \
            ( str( xml_filename ), str( error_message ) ) )
        return exclude_list
    tool_sheds = xml_tree.findall( 'repositories' )
    xml_element = []
    exclude_count = 0
    for tool_shed in tool_sheds:
        if galaxy_tool_shed_url != tool_shed.attrib[ 'tool_shed' ]:
            continue
        else:
            xml_element = tool_shed
    for reason_section in xml_element:
        reason_text = reason_section.find( 'text' ).text
        repositories = reason_section.findall( 'repository' )
        exclude_dict = dict( reason=reason_text, repositories=[] )
        for repository in repositories:
            repository_tuple = get_repository_tuple_from_elem( repository )
            if repository_tuple not in exclude_dict[ 'repositories' ]:
                exclude_verbose.append( repository_tuple )
                exclude_count += 1
                exclude_dict[ 'repositories' ].append( repository_tuple )
        exclude_list.append( exclude_dict )
    log.debug( 'The xml document %s containing the exclude list defines the following %s repositories to be excluded from testing...' % \
        ( str( xml_filename ), str( exclude_count ) ) )
    #if '-list_repositories' in sys.argv:
    for name, owner, changeset_revision in exclude_verbose:
        if changeset_revision:
            log.debug( 'Repository %s owned by %s, changeset revision %s.' % ( str( name ), str( owner ), str( changeset_revision ) ) )
        else:
            log.debug( 'Repository %s owned by %s, all revisions.' % ( str( name ), str( owner ) ) )
    return exclude_list

def register_test_result( url, tool_test_results_dicts, tool_test_results_dict, repository_dict, params, can_update_tool_shed ):
    """
    Update the repository metadata tool_test_results and appropriate flags using the Tool SHed API.  This method
    updates tool_test_results with the relevant data, sets the do_not_test and tools_functionally correct flags
    to the appropriate values and updates the time_last_tested field to the value of the received time_tested.
    """
    if can_update_tool_shed:
        metadata_revision_id = repository_dict.get( 'id', None )
        if metadata_revision_id is not None:
            tool_test_results_dicts.insert( 0, tool_test_results_dict )
            params[ 'tool_test_results' ] = tool_test_results_dicts
            # Set the time_last_tested entry so that the repository_metadata.time_last_tested will be set in the tool shed.
            params[ 'time_last_tested' ] = 'This entry will result in this value being set via the Tool Shed API.'
            url = '%s' % ( suc.url_join( galaxy_tool_shed_url,'api', 'repository_revisions', str( metadata_revision_id ) ) )
            try:
                return update( tool_shed_api_key, url, params, return_formatted=False )
            except Exception, e:
                log.exception( 'Error attempting to register test results: %s' % str( e ) )
                return {}
    else:
        return {}

def run_tests( test_config ):
    loader = nose.loader.TestLoader( config=test_config )
    test_config.plugins.addPlugin( ReportResults() )
    plug_loader = test_config.plugins.prepareTestLoader( loader )
    if plug_loader is not None:
        loader = plug_loader
    tests = loader.loadTestsFromNames( test_config.testNames )
    test_runner = nose.core.TextTestRunner( stream=test_config.stream,
                                            verbosity=test_config.verbosity,
                                            config=test_config )
    plug_runner = test_config.plugins.prepareTestRunner( test_runner )
    if plug_runner is not None:
        test_runner = plug_runner
    result = test_runner.run( tests )
    return result, test_config.plugins._plugins

def show_summary_output( repository_dicts ):
    # Group summary display by repository owner.
    repository_dicts_by_owner = {}
    for repository_dict in repository_dicts:
        name = str( repository_dict[ 'name' ] )
        owner = str( repository_dict[ 'owner' ] )
        changeset_revision = str( repository_dict[ 'changeset_revision' ] )
        if owner in repository_dicts_by_owner:
            repository_dicts_by_owner[ owner ].append( repository_dict )
        else:
            repository_dicts_by_owner[ owner ] = [ repository_dict ]
    # Display grouped summary.
    for owner, grouped_repository_dicts in repository_dicts_by_owner.items():
        print "# "
        for repository_dict in grouped_repository_dicts:
            name = str( repository_dict[ 'name' ] )
            owner = str( repository_dict[ 'owner' ] )
            changeset_revision = str( repository_dict[ 'changeset_revision' ] )
            print "# Revision %s of repository %s owned by %s" % ( changeset_revision, name, owner )

def uninstall_repository_and_repository_dependencies( app, repository_dict ):
    """Uninstall a repository and all of its repository dependencies."""
    # This method assumes that the repositor defined by the received repository_dict is not a repository
    # dependency of another repository.
    sa_session = app.install_model.context
    # The dict contains the only repository the app should have installed at this point.
    name = str( repository_dict[ 'name' ] )
    owner = str( repository_dict[ 'owner' ] )
    changeset_revision = str( repository_dict[ 'changeset_revision' ] )
    # Since this install and test framework uninstalls repositories immediately after installing and testing
    # them, the values of repository.installed_changeset_revision and repository.changeset_revision should be
    # the same.
    repository = test_db_util.get_installed_repository_by_name_owner_changeset_revision( name, owner, changeset_revision )
    if repository.can_uninstall( app ):
        # A repository can be uninstalled only if no dependent repositories are installed.  So uninstallation order
        # id critical.  A repository is always uninstalled first, and the each of its dependencies is checked to see
        # if it can be uninstalled.
        uninstall_repository_dict = dict( name=name,
                                          owner=owner,
                                          changeset_revision=changeset_revision )
        log.debug( 'Revision %s of repository %s owned by %s selected for uninstallation.' % ( changeset_revision, name, owner ) )
        try:
            test_install_repositories.generate_uninstall_method( uninstall_repository_dict )
            # Set up nose to run the generated uninstall method as a functional test.
            test_config = nose.config.Config( env=os.environ, plugins=nose.plugins.manager.DefaultPluginManager() )
            test_config.configure( sys.argv )
            # Run the uninstall method. This method uses the Galaxy web interface to uninstall the previously installed
            # repository and all of its repository dependencies, deleting each of them from disk.
            result, _ = run_tests( test_config )
            repository_uninstall_successful = result.wasSuccessful()
        except Exception, e:
            repository_uninstall_successful = False
            log.exception( 'Uninstallation of revision %s of repository %s owned by %s failed: %s.' % \
                ( rd_changeset_revision, rd_name, rd_owner, str( e ) ) )
        if repository_uninstall_successful:
            # Now that the repository is uninstalled we can attempt to uninstall each of its repository dependencies.
            # We have to do this through Twill in order to maintain app.toolbox and shed_tool_conf.xml in a state that
            # is valid for future tests.  Since some of the repository's repository dependencies may require other of
            # the repository's repository dependencies, we'll keep track of the repositories we've been able to unistall.
            processed_repository_dependency_ids = []
            while len( processed_repository_dependency_ids ) < len( repository.repository_dependencies ):
                for repository_dependency in repository.repository_dependencies:
                    if repository_dependency.id not in processed_repository_dependency_ids and repository_dependency.can_uninstall( app ):
                        processed_repository_dependency_ids.append( repository_dependency.id )
                        rd_name = str( repository_dependency.name )
                        rd_owner = str( repository_dependency.owner )
                        rd_changeset_revision = str( repository_dependency.changeset_revision )
                        uninstall_repository_dict = dict( name=rd_name,
                                                          owner=rd_owner,
                                                          changeset_revision=rd_changeset_revision )
                        log.debug( 'Revision %s of repository dependency %s owned by %s selected for uninstallation.' % \
                            ( rd_changeset_revision, rd_name, rd_owner ) )
                        # Generate a test method to uninstall the repository dependency through the embedded Galaxy application's
                        # web interface.
                        try:
                            test_install_repositories.generate_uninstall_method( uninstall_repository_dict )
                            # Set up nose to run the generated uninstall method as a functional test.
                            test_config = nose.config.Config( env=os.environ, plugins=nose.plugins.manager.DefaultPluginManager() )
                            test_config.configure( sys.argv )
                            # Run the uninstall method.
                            result, _ = run_tests( test_config )
                            if not result.wasSuccessful():
                                # We won't set ok here because we'll continue to uninstall whatever we can.
                                log.debug( 'Uninstallation of revision %s of repository %s owned by %s failed.' % \
                                    ( rd_changeset_revision, rd_name, rd_owner ) )
                        except Exception, e:
                            log.exception( 'Uninstallation of revision %s of repository %s owned by %s failed: %s.' % \
                                ( rd_changeset_revision, rd_name, rd_owner, str( e ) ) )
        else:
            log.debug( 'Uninstallation of revision %s of repository %s owned by %s failed.' % ( changeset_revision, name, owner ) )
    else:
        log_reason_repository_cannot_be_uninstalled( app, repository )

def uninstall_tool_dependency( app, tool_dependency ):
    """Attempt to uninstall a tool dependency."""
    sa_session = app.install_model.context
    # Clean out any generated tests. This is necessary for Twill.
    tool_dependency_install_path = tool_dependency.installation_directory( app )
    uninstalled, error_message = tool_dependency_util.remove_tool_dependency( app, tool_dependency )
    if error_message:
        log.debug( 'There was an error attempting to remove directory: %s' % str( tool_dependency_install_path ) )
        log.debug( error_message )
    else:
        log.debug( 'Successfully removed tool dependency installation directory: %s' % str( tool_dependency_install_path ) )
    if not uninstalled or tool_dependency.status != app.model.ToolDependency.installation_status.UNINSTALLED:
        tool_dependency.status = app.model.ToolDependency.installation_status.UNINSTALLED
        sa_session.add( tool_dependency )
        sa_session.flush()
    if os.path.exists( tool_dependency_install_path ):
       log.debug( 'Uninstallation of tool dependency succeeded, but the installation path still exists on the filesystem. It is now being explicitly deleted.') 
       suc.remove_dir( tool_dependency_install_path )

