<?xml version="1.0"?>
<job_conf>
    <plugins workers="4">
        <!-- "workers" is the number of threads for the runner's work queue.
             The default from <plugins> is used if not defined for a <plugin>.
          -->
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="pbs" type="runner" load="galaxy.jobs.runners.pbs:PBSJobRunner" workers="2"/>
        <plugin id="drmaa" type="runner" load="galaxy.jobs.runners.drmaa:DRMAAJobRunner">
            <!-- Different DRMs handle successfully completed jobs differently,
                 these options can be changed to handle such differences and
                 are explained in detail on the Galaxy wiki. Defaults are shown -->
            <param id="invalidjobexception_state">ok</param>
            <param id="invalidjobexception_retries">0</param>
            <param id="internalexception_state">ok</param>
            <param id="internalexception_retries">0</param>
        </plugin>
        <plugin id="sge" type="runner" load="galaxy.jobs.runners.drmaa:DRMAAJobRunner">
            <!-- Override the $DRMAA_LIBRARY_PATH environment variable -->
            <param id="drmaa_library_path">/sge/lib/libdrmaa.so</param>
        </plugin>
        <plugin id="lwr" type="runner" load="galaxy.jobs.runners.lwr:LwrJobRunner">
          <!-- More information on LWR can be found at https://lwr.readthedocs.org -->
          <!-- Uncomment following line to use libcurl to perform HTTP calls (defaults to urllib) -->
          <!-- <param id="transport">curl</param> -->
          <!-- *Experimental Caching*: Uncomment next parameters to enable
               caching and specify the number of caching threads to enable on Galaxy
               side. Likely will not work with newer features such as MQ support.
               If this is enabled be sure to specify a `file_cache_dir` in the remote
               LWR's main configuration file.
          -->
          <!-- <param id="cache">True</param> -->
          <!-- <param id="transfer_threads">2</param> -->
        </plugin>
        <plugin id="amqp_lwr" type="runner" load="galaxy.jobs.runners.lwr:LwrJobRunner">
          <param id="url">amqp://guest:guest@localhost:5672//</param>
          <!-- If using message queue driven LWR - the LWR will generally
               initiate file transfers so a the URL of this Galaxy instance
               must be configured. -->
          <param id="galaxy_url">http://localhost:8080/</param>
          <!-- If multiple managers configured on the LWR, specify which one
               this plugin targets. -->
          <!-- <param id="manager">_default_</param> -->
          <!-- The AMQP client can provide an SSL client certificate (e.g. for
               validation), the following options configure that certificate
               (see for reference:
                 http://kombu.readthedocs.org/en/latest/reference/kombu.connection.html
               ). If you simply want to use SSL but not use/validate a client
               cert, just use the ?ssl=1 query on the amqp URL instead. -->
          <!-- <param id="amqp_connect_ssl_ca_certs">/path/to/cacert.pem</param> -->
          <!-- <param id="amqp_connect_ssl_keyfile">/path/to/key.pem</param> -->
          <!-- <param id="amqp_connect_ssl_certfile">/path/to/cert.pem</param> -->
          <!-- <param id="amqp_connect_ssl_cert_reqs">cert_required</param> -->
          <!-- By default, the AMQP consumer uses a nonblocking connection with
               a 0.2 second timeout. In testing, this works fine for
               unencrypted AMQP connections, but with SSL it will cause the
               client to reconnect to the server after each timeout. Set to a
               higher value (in seconds) (or `None` to use blocking connections). -->
          <!-- <param id="amqp_consumer_timeout">None</param> -->
        </plugin>
        <plugin id="cli" type="runner" load="galaxy.jobs.runners.cli:ShellJobRunner" />
        <plugin id="condor" type="runner" load="galaxy.jobs.runners.condor:CondorJobRunner" />
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner" />
    </plugins>
    <handlers default="handlers">
        <!-- Additional job handlers - the id should match the name of a
             [server:<id>] in universe_wsgi.ini.
         -->
        <handler id="handler0" tags="handlers"/>
        <handler id="handler1" tags="handlers"/>
        <!-- Handlers will load all plugins defined in the <plugins> collection
             above by default, but can be limited to a subset using <plugin>
             tags. This is useful for heterogenous environments where the DRMAA
             plugin would need to be loaded more than once with different
             configs.
         -->
        <handler id="sge_handler">
            <plugin id="sge"/>
        </handler>
        <handler id="special_handler0" tags="special_handlers"/>
        <handler id="special_handler1" tags="special_handlers"/>
        <handler id="trackster_handler"/>
    </handlers>
    <destinations default="local">
        <!-- Destinations define details about remote resources and how jobs
             should be executed on those remote resources.
         -->
        <destination id="local" runner="local"/>
        <destination id="multicore_local" runner="local">
          <param id="local_slots">4</param> <!-- Specify GALAXY_SLOTS for local jobs. -->
          <!-- Warning: Local slot count doesn't tie up additional worker threads, to prevent over 
               allocating machine define a second local runner with different name and fewer workers
               to run this destination. -->
          <job_metrics /> 
          <!-- Above element demonstrates embedded job metrics definition - see
               job_metrics_conf.xml.sample for full documentation on possible nested
               elements. This empty block will simply disable job metrics for the
               corresponding destination. -->
        </destination>
        <destination id="pbs" runner="pbs" tags="mycluster"/>
        <destination id="pbs_longjobs" runner="pbs" tags="mycluster,longjobs">
            <!-- Define parameters that are native to the job runner plugin. -->
            <param id="Resource_List">walltime=72:00:00</param>
        </destination>
        <destination id="remote_cluster" runner="drmaa" tags="longjobs"/>
        <destination id="java_cluster" runner="drmaa">
          <!-- set arbitrary environment variables at runtime - like metrics
               doesn't yet work with local or CLI runners. But should work with
               DRMAA/SLURM, PBS, Condor, and LWR. General dependencies for tools
               should be configured via tool_dependency_dir and package options
               and these options should be reserved for defining cluster specific
               options.
          -->
          <env id="_JAVA_OPTIONS">-Xmx=6GB</env>
          <env id="ANOTHER_OPTION" raw="true">'5'</env> <!-- raw disables auto quoting -->
          <env file="/mnt/java_cluster/environment_setup.sh" /> <!-- will be sourced -->
          <env exec="module load javastuff/2.10" /> <!-- will be sourced -->
          <!-- files to source and exec statements will be handled on remote
               clusters. These don't need to be available on the Galaxy server
               itself.
          -->
        </destination>
        <destination id="real_user_cluster" runner="drmaa">
            <!-- TODO: The real user options should maybe not be considered runner params. -->
            <param id="galaxy_external_runjob_script">scripts/drmaa_external_runner.py</param>
            <param id="galaxy_external_killjob_script">scripts/drmaa_external_killer.py</param>
            <param id="galaxy_external_chown_script">scripts/external_chown_script.py</param>
        </destination>
        <destination id="dynamic" runner="dynamic">
            <!-- A destination that represents a method in the dynamic runner. -->
            <param id="function">foo</param>
        </destination>
        <destination id="secure_lwr" runner="lwr">
            <param id="url">https://windowshost.examle.com:8913/</param>
            <!-- If set, private_token must match token remote LWR server configured with. -->
            <param id="private_token">123456789changeme</param>
            <!-- Uncomment the following statement to disable file staging (e.g.
                 if there is a shared file system between Galaxy and the LWR 
                 server). Alternatively action can be set to 'copy' - to replace 
                 http transfers with file system copies, 'remote_transfer' to cause
                 the lwr to initiate HTTP transfers instead of Galaxy, or
                 'remote_copy' to cause lwr to initiate file system copies.
                 If setting this to 'remote_transfer' be sure to specify a
                 'galaxy_url' attribute on the runner plugin above. -->
            <!-- <param id="default_file_action">none</param> -->
            <!-- The above option is just the default, the transfer behavior
                 none|copy|http can be configured on a per path basis via the
                 following file. See lib/galaxy/jobs/runners/lwr_client/action_mapper.py
                 for examples of how to configure this file. This is very beta
                 and nature of file will likely change.
            -->
            <!-- <param id="file_action_config">file_actions.json</param> -->
            <!-- Uncomment following option to disable Galaxy tool dependency
                 resolution and utilize remote LWR's configuraiton of tool
                 dependency resolution instead (same options as Galaxy for
                 dependency resolution are available in LWR). At a minimum
                 the remote LWR server should define a tool_dependencies_dir in
                 its `server.ini` configuration. The LWR will not attempt to
                 stage dependencies - so ensure the the required galaxy or tool
                 shed packages are available remotely (exact same tool shed
                 installed changesets are required).
            -->
            <!-- <param id="dependency_resolution">remote</params> -->
            <!-- Traditionally, the LWR allow Galaxy to generate a command line
                 as if it were going to run the command locally and then the
                 LWR client rewrites it after the fact using regular
                 expressions. Setting the following value to true causes the
                 LWR runner to insert itself into the command line generation
                 process and generate the correct command line from the get go.
                 This will likely be the default someday - but requires a newer
                 LWR version and is less well tested. -->
            <!-- <param id="rewrite_parameters">true</params> -->
            <!-- Uncomment following option to enable setting metadata on remote
                 LWR server. The 'use_remote_datatypes' option is available for
                 determining whether to use remotely configured datatypes or local
                 ones (both alternatives are a little brittle). -->
            <!-- <param id="remote_metadata">true</param> -->
            <!-- <param id="use_remote_datatypes">false</param> -->
            <!-- <param id="remote_property_galaxy_home">/path/to/remote/galaxy-central</param> -->
            <!-- If remote LWR server is configured to run jobs as the real user,
                 uncomment the following line to pass the current Galaxy user
                 along. -->
            <!-- <param id="submit_user">$__user_name__</param> -->
            <!-- Various other submission parameters can be passed along to the LWR
                 whose use will depend on the remote LWR's configured job manager.
                 For instance:
            -->
            <!-- <param id="submit_native_specification">-P bignodes -R y -pe threads 8</param> -->            
        </destination>
        <destination id="amqp_lwr_dest" runner="amqp_lwr" >
            <!-- url and private_token are not valid when using MQ driven LWR. The plugin above
                 determines which queue/manager to target and the underlying MQ server should be
                 used to configure security.
            -->
            <!-- Traditionally, the LWR client sends request to LWR
                 server to populate various system properties. This
                 extra step can be disabled and these calculated here
                 on client by uncommenting jobs_directory and
                 specifying any additional remote_property_ of
                 interest, this is not optional when using message
                 queues.
            -->
            <param id="jobs_directory">/path/to/remote/lwr/lwr_staging/</param>
            <!-- Default the LWR send files to and pull files from Galaxy when
                 using message queues (in the more traditional mode Galaxy sends
                 files to and pull files from the LWR - this is obviously less
                 appropriate when using a message queue).
            -->
            <param id="default_file_action">remote_transfer</param>
        </destination>
        <destination id="ssh_torque" runner="cli">
            <param id="shell_plugin">SecureShell</param>
            <param id="job_plugin">Torque</param>
            <param id="shell_username">foo</param>
            <param id="shell_hostname">foo.example.org</param>
            <param id="job_Resource_List">walltime=24:00:00,ncpus=4</param>
        </destination>

        <!-- Example CLI Slurm runner. -->
        <destination id="ssh_slurm" runner="cli">
            <param id="shell_plugin">SecureShell</param>
            <param id="job_plugin">Slurm</param>
            <param id="shell_username">foo</param>
            <param id="shell_hostname">my_host</param>
            <param id="job_time">2:00:00</param>
            <param id="job_ncpus">4</param>
            <param id="job_partition">my_partition</param>
        </destination>

        <destination id="condor" runner="condor">
            <!-- With no params, jobs are submitted to the 'vanilla' universe with:
                    notification = NEVER
                    getenv = true
                 Additional/override query ClassAd params can be specified with
                 <param> tags.
            -->
            <param id="request_cpus">8</param>
        </destination>
    </destinations>
    <tools>
        <!-- Tools can be configured to use specific destinations or handlers,
             identified by either the "id" or "tags" attribute.  If assigned to
             a tag, a handler or destination that matches that tag will be
             chosen at random.
         -->
        <tool id="foo" handler="trackster_handler">
            <param id="source">trackster</param>
        </tool>
        <tool id="bar" destination="dynamic"/>
        <tool id="baz" handler="special_handlers" destination="bigmem"/>
    </tools>
    <limits>
        <!-- Certain limits can be defined. -->
        <limit type="registered_user_concurrent_jobs">2</limit>
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <limit type="walltime">24:00:00</limit>
        <limit type="concurrent_jobs" id="local">1</limit>
        <limit type="concurrent_jobs" tag="mycluster">2</limit>
        <limit type="concurrent_jobs" tag="longjobs">1</limit>
    </limits>
</job_conf>
